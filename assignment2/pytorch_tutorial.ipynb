{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Training an MNIST Classifier\n",
    "=====\n",
    "## Custom Dataset, Model Checkpointing, and Fine-tune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import glob\n",
    "import os.path as osp\n",
    "import numpy as np\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Custom Dataset\n",
    "PyTorch has many built-in datasets such as MNIST and CIFAR. In this tutorial, we demonstrate how to write your own dataset by implementing a custom MNIST dataset class. Use [this link](https://github.com/myleott/mnist_png/blob/master/mnist_png.tar.gz?raw=true) to download the mnist png dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MNIST(Dataset):\n",
    "    \"\"\"\n",
    "    A customized data loader for MNIST.\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 root,\n",
    "                 transform=None,\n",
    "                 preload=False):\n",
    "        \"\"\" Intialize the MNIST dataset\n",
    "        \n",
    "        Args:\n",
    "            - root: root directory of the dataset\n",
    "            - tranform: a custom tranform function\n",
    "            - preload: if preload the dataset into memory\n",
    "        \"\"\"\n",
    "        self.images = None\n",
    "        self.labels = None\n",
    "        self.filenames = []\n",
    "        self.root = root\n",
    "        self.transform = transform\n",
    "\n",
    "        # read filenames\n",
    "        for i in range(10):\n",
    "            filenames = glob.glob(osp.join(root, str(i), '*.png'))\n",
    "            for fn in filenames:\n",
    "                self.filenames.append((fn, i)) # (filename, label) pair\n",
    "                \n",
    "        # if preload dataset into memory\n",
    "        if preload:\n",
    "            self._preload()\n",
    "            \n",
    "        self.len = len(self.filenames)\n",
    "                              \n",
    "    def _preload(self):\n",
    "        \"\"\"\n",
    "        Preload dataset to memory\n",
    "        \"\"\"\n",
    "        self.labels = []\n",
    "        self.images = []\n",
    "        for image_fn, label in self.filenames:            \n",
    "            # load images\n",
    "            image = Image.open(image_fn)\n",
    "            # avoid too many opened files bug\n",
    "            self.images.append(image.copy())\n",
    "            image.close()\n",
    "            self.labels.append(label)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \"\"\" Get a sample from the dataset\n",
    "        \"\"\"\n",
    "        if self.images is not None:\n",
    "            # If dataset is preloaded\n",
    "            image = self.images[index]\n",
    "            label = self.labels[index]\n",
    "        else:\n",
    "            # If on-demand data loading\n",
    "            image_fn, label = self.filenames[index]\n",
    "            image = Image.open(image_fn)\n",
    "            \n",
    "        # May use transform function to transform samples\n",
    "        # e.g., random crop, whitening\n",
    "        if self.transform is not None:\n",
    "            image = self.transform(image)\n",
    "        # return image and label\n",
    "        return image, label\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        Total number of samples in the dataset\n",
    "        \"\"\"\n",
    "        return self.len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the MNIST dataset. \n",
    "# transforms.ToTensor() automatically converts PIL images to\n",
    "# torch tensors with range [0, 1]\n",
    "trainset = MNIST(\n",
    "    root='mnist_png/training',\n",
    "    preload=True, transform=transforms.ToTensor(),\n",
    ")\n",
    "# Use the torch dataloader to iterate through the dataset\n",
    "trainset_loader = DataLoader(trainset, batch_size=64, shuffle=True, num_workers=1)\n",
    "\n",
    "# load the testset\n",
    "testset = MNIST(\n",
    "    root='mnist_png/testing',\n",
    "    preload=True, transform=transforms.ToTensor(),\n",
    ")\n",
    "# Use the torch dataloader to iterate through the dataset\n",
    "testset_loader = DataLoader(testset, batch_size=1000, shuffle=False, num_workers=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60000\n",
      "10000\n"
     ]
    }
   ],
   "source": [
    "print(len(trainset))\n",
    "print(len(testset))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# functions to show an image\n",
    "def imshow(img):\n",
    "    npimg = img.numpy()\n",
    "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
    "\n",
    "# get some random training images\n",
    "dataiter = iter(trainset_loader)\n",
    "images, labels = dataiter.next()\n",
    "\n",
    "# show images\n",
    "imshow(torchvision.utils.make_grid(images))\n",
    "# print labels\n",
    "print(' '.join('%5s' % labels[j] for j in range(16)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use GPU if available, otherwise stick with cpu\n",
    "use_cuda = torch.cuda.is_available()\n",
    "torch.manual_seed(123)\n",
    "device = torch.device('cuda' if use_cuda else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define a Conv Net\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 10, kernel_size=5)\n",
    "        self.conv2 = nn.Conv2d(10, 20, kernel_size=5)\n",
    "        self.conv2_drop = nn.Dropout2d()\n",
    "        self.fc1 = nn.Linear(320, 50)\n",
    "        self.fc2 = nn.Linear(50, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(F.max_pool2d(self.conv1(x), 2))\n",
    "        x = F.relu(F.max_pool2d(self.conv2_drop(self.conv2(x)), 2))\n",
    "        x = x.view(-1, 320)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        x = self.fc2(x)\n",
    "        return F.log_softmax(x, dim=1)\n",
    "\n",
    "model = Net().to(device)\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch, log_interval=100):\n",
    "    model.train()  # set training mode\n",
    "    iteration = 0\n",
    "    for ep in range(epoch):\n",
    "        for batch_idx, (data, target) in enumerate(trainset_loader):\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            output = model(data)\n",
    "            loss = F.nll_loss(output, target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            if iteration % log_interval == 0:\n",
    "                print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                    ep, batch_idx * len(data), len(trainset_loader.dataset),\n",
    "                    100. * batch_idx / len(trainset_loader), loss.item()))\n",
    "            iteration += 1\n",
    "        test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test():\n",
    "    model.eval()  # set evaluation mode\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in testset_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            test_loss += F.nll_loss(output, target, size_average=False).item() # sum up batch loss\n",
    "            pred = output.max(1, keepdim=True)[1] # get the index of the max log-probability\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "    test_loss /= len(testset_loader.dataset)\n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        test_loss, correct, len(testset_loader.dataset),\n",
    "        100. * correct / len(testset_loader.dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 0 [0/60000 (0%)]\tLoss: 2.332354\n",
      "Train Epoch: 0 [6400/60000 (11%)]\tLoss: 2.288828\n",
      "Train Epoch: 0 [12800/60000 (21%)]\tLoss: 2.274398\n",
      "Train Epoch: 0 [19200/60000 (32%)]\tLoss: 2.257984\n",
      "Train Epoch: 0 [25600/60000 (43%)]\tLoss: 2.229742\n",
      "Train Epoch: 0 [32000/60000 (53%)]\tLoss: 2.025782\n",
      "Train Epoch: 0 [38400/60000 (64%)]\tLoss: 1.905468\n",
      "Train Epoch: 0 [44800/60000 (75%)]\tLoss: 1.905217\n",
      "Train Epoch: 0 [51200/60000 (85%)]\tLoss: 1.481127\n",
      "Train Epoch: 0 [57600/60000 (96%)]\tLoss: 1.149691\n",
      "\n",
      "Test set: Average loss: 0.7354, Accuracy: 7957/10000 (80%)\n",
      "\n",
      "Train Epoch: 1 [3968/60000 (7%)]\tLoss: 0.771532\n",
      "Train Epoch: 1 [10368/60000 (17%)]\tLoss: 0.447568\n",
      "Train Epoch: 1 [16768/60000 (28%)]\tLoss: 0.360693\n",
      "Train Epoch: 1 [23168/60000 (39%)]\tLoss: 0.387261\n",
      "Train Epoch: 1 [29568/60000 (49%)]\tLoss: 0.309941\n",
      "Train Epoch: 1 [35968/60000 (60%)]\tLoss: 0.339990\n",
      "Train Epoch: 1 [42368/60000 (71%)]\tLoss: 0.351554\n",
      "Train Epoch: 1 [48768/60000 (81%)]\tLoss: 0.294428\n",
      "Train Epoch: 1 [55168/60000 (92%)]\tLoss: 0.211168\n",
      "\n",
      "Test set: Average loss: 0.2454, Accuracy: 9282/10000 (93%)\n",
      "\n",
      "Train Epoch: 2 [1536/60000 (3%)]\tLoss: 0.205518\n",
      "Train Epoch: 2 [7936/60000 (13%)]\tLoss: 0.221640\n",
      "Train Epoch: 2 [14336/60000 (24%)]\tLoss: 0.331802\n",
      "Train Epoch: 2 [20736/60000 (35%)]\tLoss: 0.226940\n",
      "Train Epoch: 2 [27136/60000 (45%)]\tLoss: 0.284527\n",
      "Train Epoch: 2 [33536/60000 (56%)]\tLoss: 0.304491\n",
      "Train Epoch: 2 [39936/60000 (67%)]\tLoss: 0.124944\n",
      "Train Epoch: 2 [46336/60000 (77%)]\tLoss: 0.325482\n",
      "Train Epoch: 2 [52736/60000 (88%)]\tLoss: 0.186691\n",
      "Train Epoch: 2 [59136/60000 (99%)]\tLoss: 0.231756\n",
      "\n",
      "Test set: Average loss: 0.1608, Accuracy: 9505/10000 (95%)\n",
      "\n",
      "Train Epoch: 3 [5504/60000 (9%)]\tLoss: 0.088741\n",
      "Train Epoch: 3 [11904/60000 (20%)]\tLoss: 0.191312\n",
      "Train Epoch: 3 [18304/60000 (30%)]\tLoss: 0.145058\n",
      "Train Epoch: 3 [24704/60000 (41%)]\tLoss: 0.105362\n",
      "Train Epoch: 3 [31104/60000 (52%)]\tLoss: 0.086197\n",
      "Train Epoch: 3 [37504/60000 (62%)]\tLoss: 0.470696\n",
      "Train Epoch: 3 [43904/60000 (73%)]\tLoss: 0.110062\n",
      "Train Epoch: 3 [50304/60000 (84%)]\tLoss: 0.138312\n",
      "Train Epoch: 3 [56704/60000 (94%)]\tLoss: 0.098289\n",
      "\n",
      "Test set: Average loss: 0.1286, Accuracy: 9594/10000 (96%)\n",
      "\n",
      "Train Epoch: 4 [3072/60000 (5%)]\tLoss: 0.193768\n",
      "Train Epoch: 4 [9472/60000 (16%)]\tLoss: 0.056981\n",
      "Train Epoch: 4 [15872/60000 (26%)]\tLoss: 0.135394\n",
      "Train Epoch: 4 [22272/60000 (37%)]\tLoss: 0.064215\n",
      "Train Epoch: 4 [28672/60000 (48%)]\tLoss: 0.110289\n",
      "Train Epoch: 4 [35072/60000 (58%)]\tLoss: 0.090841\n",
      "Train Epoch: 4 [41472/60000 (69%)]\tLoss: 0.046409\n",
      "Train Epoch: 4 [47872/60000 (80%)]\tLoss: 0.100231\n",
      "Train Epoch: 4 [54272/60000 (90%)]\tLoss: 0.118323\n",
      "\n",
      "Test set: Average loss: 0.1046, Accuracy: 9672/10000 (97%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train(5)  # train 5 epochs should get you to about 97% accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Save the model (model checkpointing)\n",
    "\n",
    "Now we have a model! Obviously we do not want to retrain the model everytime we want to use it. Plus if you are training a super big model, you probably want to save checkpoint periodically so that you can always fall back to the last checkpoint in case something bad happened or you simply want to test models at different training iterations.\n",
    "\n",
    "Model checkpointing is fairly simple in PyTorch. First, we define a helper function that can save a model to the disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_checkpoint(checkpoint_path, model, optimizer):\n",
    "    state = {'state_dict': model.state_dict(),\n",
    "             'optimizer' : optimizer.state_dict()}\n",
    "    torch.save(state, checkpoint_path)\n",
    "    print('model saved to %s' % checkpoint_path)\n",
    "    \n",
    "def load_checkpoint(checkpoint_path, model, optimizer):\n",
    "    state = torch.load(checkpoint_path)\n",
    "    model.load_state_dict(state['state_dict'])\n",
    "    optimizer.load_state_dict(state['optimizer'])\n",
    "    print('model loaded from %s' % checkpoint_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Average loss: 2.3051, Accuracy: 953/10000 (10%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# create a brand new model\n",
    "model = Net().to(device)\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
    "test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define a training loop with model checkpointing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_save(epoch, save_interval, log_interval=100):\n",
    "    model.train()  # set training mode\n",
    "    iteration = 0\n",
    "    for ep in range(epoch):\n",
    "        for batch_idx, (data, target) in enumerate(trainset_loader):\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            output = model(data)\n",
    "            loss = F.nll_loss(output, target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            if iteration % log_interval == 0:\n",
    "                print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                    ep, batch_idx * len(data), len(trainset_loader.dataset),\n",
    "                    100. * batch_idx / len(trainset_loader), loss.item()))\n",
    "            if iteration % save_interval == 0 and iteration > 0:\n",
    "                save_checkpoint('mnist-%i.pth' % iteration, model, optimizer)\n",
    "            iteration += 1\n",
    "        test()\n",
    "    \n",
    "    # save the final model\n",
    "    save_checkpoint('mnist-%i.pth' % iteration, model, optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 0 [0/60000 (0%)]\tLoss: 2.307217\n",
      "Train Epoch: 0 [6400/60000 (11%)]\tLoss: 2.294364\n",
      "Train Epoch: 0 [12800/60000 (21%)]\tLoss: 2.295774\n",
      "Train Epoch: 0 [19200/60000 (32%)]\tLoss: 2.284744\n",
      "Train Epoch: 0 [25600/60000 (43%)]\tLoss: 2.248438\n",
      "Train Epoch: 0 [32000/60000 (53%)]\tLoss: 2.218840\n",
      "model saved to mnist-500.pth\n",
      "Train Epoch: 0 [38400/60000 (64%)]\tLoss: 2.119733\n",
      "Train Epoch: 0 [44800/60000 (75%)]\tLoss: 1.858666\n",
      "Train Epoch: 0 [51200/60000 (85%)]\tLoss: 1.578590\n",
      "Train Epoch: 0 [57600/60000 (96%)]\tLoss: 1.237142\n",
      "\n",
      "Test set: Average loss: 0.7952, Accuracy: 8111/10000 (81%)\n",
      "\n",
      "Train Epoch: 1 [3968/60000 (7%)]\tLoss: 0.380946\n",
      "model saved to mnist-1000.pth\n",
      "Train Epoch: 1 [10368/60000 (17%)]\tLoss: 0.284827\n",
      "Train Epoch: 1 [16768/60000 (28%)]\tLoss: 0.518267\n",
      "Train Epoch: 1 [23168/60000 (39%)]\tLoss: 0.344647\n",
      "Train Epoch: 1 [29568/60000 (49%)]\tLoss: 0.497286\n",
      "Train Epoch: 1 [35968/60000 (60%)]\tLoss: 0.287575\n",
      "model saved to mnist-1500.pth\n",
      "Train Epoch: 1 [42368/60000 (71%)]\tLoss: 0.322255\n",
      "Train Epoch: 1 [48768/60000 (81%)]\tLoss: 0.260835\n",
      "Train Epoch: 1 [55168/60000 (92%)]\tLoss: 0.249864\n",
      "\n",
      "Test set: Average loss: 0.2524, Accuracy: 9217/10000 (92%)\n",
      "\n",
      "Train Epoch: 2 [1536/60000 (3%)]\tLoss: 0.129708\n",
      "Train Epoch: 2 [7936/60000 (13%)]\tLoss: 0.192912\n",
      "model saved to mnist-2000.pth\n",
      "Train Epoch: 2 [14336/60000 (24%)]\tLoss: 0.279395\n",
      "Train Epoch: 2 [20736/60000 (35%)]\tLoss: 0.226438\n",
      "Train Epoch: 2 [27136/60000 (45%)]\tLoss: 0.249021\n",
      "Train Epoch: 2 [33536/60000 (56%)]\tLoss: 0.311160\n",
      "Train Epoch: 2 [39936/60000 (67%)]\tLoss: 0.401957\n",
      "model saved to mnist-2500.pth\n",
      "Train Epoch: 2 [46336/60000 (77%)]\tLoss: 0.299187\n",
      "Train Epoch: 2 [52736/60000 (88%)]\tLoss: 0.104542\n",
      "Train Epoch: 2 [59136/60000 (99%)]\tLoss: 0.053371\n",
      "\n",
      "Test set: Average loss: 0.1520, Accuracy: 9548/10000 (95%)\n",
      "\n",
      "Train Epoch: 3 [5504/60000 (9%)]\tLoss: 0.150327\n",
      "Train Epoch: 3 [11904/60000 (20%)]\tLoss: 0.207914\n",
      "model saved to mnist-3000.pth\n",
      "Train Epoch: 3 [18304/60000 (30%)]\tLoss: 0.039575\n",
      "Train Epoch: 3 [24704/60000 (41%)]\tLoss: 0.138774\n",
      "Train Epoch: 3 [31104/60000 (52%)]\tLoss: 0.164419\n",
      "Train Epoch: 3 [37504/60000 (62%)]\tLoss: 0.051587\n",
      "Train Epoch: 3 [43904/60000 (73%)]\tLoss: 0.247406\n",
      "model saved to mnist-3500.pth\n",
      "Train Epoch: 3 [50304/60000 (84%)]\tLoss: 0.049218\n",
      "Train Epoch: 3 [56704/60000 (94%)]\tLoss: 0.113666\n",
      "\n",
      "Test set: Average loss: 0.1178, Accuracy: 9636/10000 (96%)\n",
      "\n",
      "Train Epoch: 4 [3072/60000 (5%)]\tLoss: 0.082211\n",
      "Train Epoch: 4 [9472/60000 (16%)]\tLoss: 0.065687\n",
      "Train Epoch: 4 [15872/60000 (26%)]\tLoss: 0.087543\n",
      "model saved to mnist-4000.pth\n",
      "Train Epoch: 4 [22272/60000 (37%)]\tLoss: 0.356242\n",
      "Train Epoch: 4 [28672/60000 (48%)]\tLoss: 0.089219\n",
      "Train Epoch: 4 [35072/60000 (58%)]\tLoss: 0.110018\n",
      "Train Epoch: 4 [41472/60000 (69%)]\tLoss: 0.074840\n",
      "Train Epoch: 4 [47872/60000 (80%)]\tLoss: 0.155238\n",
      "model saved to mnist-4500.pth\n",
      "Train Epoch: 4 [54272/60000 (90%)]\tLoss: 0.155041\n",
      "\n",
      "Test set: Average loss: 0.0946, Accuracy: 9698/10000 (97%)\n",
      "\n",
      "model saved to mnist-4690.pth\n"
     ]
    }
   ],
   "source": [
    "train_save(5, 500, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model loaded from mnist-4690.pth\n",
      "\n",
      "Test set: Average loss: 0.0946, Accuracy: 9698/10000 (97%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# create a new model\n",
    "model = Net().to(device)\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
    "# load from the final checkpoint\n",
    "load_checkpoint('mnist-4690.pth', model, optimizer)\n",
    "# should give you the final model accuracy\n",
    "test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Fine-tune a model\n",
    "\n",
    "Sometimes you want to fine-tune a pretrained model instead of training a model from scratch. For example, if you want to train a model on a new dataset that contains natural images. To achieve the best performance, you can start with a model that's fully trained on ImageNet and fine-tune the model.\n",
    "\n",
    "Finetuning a model in PyTorch is super easy! First, let's find out what we saved in a checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias'])\n"
     ]
    }
   ],
   "source": [
    "# What's in a state dict?\n",
    "print(model.state_dict().keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finetune the fc layers\n",
    "\n",
    "Now say we want to load the conv layers from the checkpoint and train the fc layers. We can simply load a subset of the state dict with the selected names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = torch.load('mnist-4690.pth')\n",
    "states_to_load = {}\n",
    "for name, param in checkpoint['state_dict'].items():\n",
    "    if name.startswith('conv'):\n",
    "        states_to_load[name] = param\n",
    "\n",
    "# Construct a new state dict in which the layers we want\n",
    "# to import from the checkpoint is update with the parameters\n",
    "# from the checkpoint\n",
    "model_state = model.state_dict()\n",
    "model_state.update(states_to_load)\n",
    "        \n",
    "model = Net().to(device)\n",
    "model.load_state_dict(model_state)\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 0 [0/60000 (0%)]\tLoss: 4.512208\n",
      "Train Epoch: 0 [6400/60000 (11%)]\tLoss: 0.841220\n",
      "Train Epoch: 0 [12800/60000 (21%)]\tLoss: 0.800756\n",
      "Train Epoch: 0 [19200/60000 (32%)]\tLoss: 0.589858\n",
      "Train Epoch: 0 [25600/60000 (43%)]\tLoss: 0.393606\n",
      "Train Epoch: 0 [32000/60000 (53%)]\tLoss: 0.648841\n",
      "Train Epoch: 0 [38400/60000 (64%)]\tLoss: 0.452495\n",
      "Train Epoch: 0 [44800/60000 (75%)]\tLoss: 0.380981\n",
      "Train Epoch: 0 [51200/60000 (85%)]\tLoss: 0.379492\n",
      "Train Epoch: 0 [57600/60000 (96%)]\tLoss: 0.569643\n",
      "\n",
      "Test set: Average loss: 0.1621, Accuracy: 9517/10000 (95%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train(1)  # training 1 epoch will get you to 93%!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import pretrained weights in a different model\n",
    "\n",
    "We can even use the pretrained conv layers in a different model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SmallNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SmallNet, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 10, kernel_size=5)\n",
    "        self.conv2 = nn.Conv2d(10, 20, kernel_size=5)\n",
    "        self.conv2_drop = nn.Dropout2d()\n",
    "        self.fc1 = nn.Linear(320, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(F.max_pool2d(self.conv1(x), 2))\n",
    "        x = F.relu(F.max_pool2d(self.conv2_drop(self.conv2(x)), 2))\n",
    "        x = x.view(-1, 320)\n",
    "        x = self.fc1(x)\n",
    "        return F.log_softmax(x, dim=1)\n",
    "\n",
    "model = SmallNet().to(device)\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = torch.load('mnist-4690.pth')\n",
    "states_to_load = {}\n",
    "for name, param in checkpoint['state_dict'].items():\n",
    "    if name.startswith('conv'):\n",
    "        states_to_load[name] = param\n",
    "\n",
    "# Construct a new state dict in which the layers we want\n",
    "# to import from the checkpoint is update with the parameters\n",
    "# from the checkpoint\n",
    "model_state = model.state_dict()\n",
    "model_state.update(states_to_load)\n",
    "        \n",
    "model.load_state_dict(model_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 0 [0/60000 (0%)]\tLoss: 5.455848\n",
      "Train Epoch: 0 [6400/60000 (11%)]\tLoss: 0.697587\n",
      "Train Epoch: 0 [12800/60000 (21%)]\tLoss: 0.403508\n",
      "Train Epoch: 0 [19200/60000 (32%)]\tLoss: 0.420885\n",
      "Train Epoch: 0 [25600/60000 (43%)]\tLoss: 0.331131\n",
      "Train Epoch: 0 [32000/60000 (53%)]\tLoss: 0.338215\n",
      "Train Epoch: 0 [38400/60000 (64%)]\tLoss: 0.426086\n",
      "Train Epoch: 0 [44800/60000 (75%)]\tLoss: 0.198424\n",
      "Train Epoch: 0 [51200/60000 (85%)]\tLoss: 0.339682\n",
      "Train Epoch: 0 [57600/60000 (96%)]\tLoss: 0.313183\n",
      "\n",
      "Test set: Average loss: 0.1692, Accuracy: 9470/10000 (95%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train(1)  # training 1 epoch will get you to 93%!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
